% Chapter Template

\chapter{The Use of Semi-supervised Learning in Classification of English Grammatical Structures} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Introduction}

The need to assess the difficulty of written texts is crucial in foreign language learning and teaching. Language instructors need a text difficulty metric that conforms to the guidelines of foreign language learning and teaching to find reading materials that match the proficiency level of their learners. Similarly, curricula designers and textbook publishers need such a metric to provide reliable and consistent learning resources and activities. Existing text evaluation metrics (\cite{kincaid1975flesch}, \cite{stenner2006accurate}, \cite{landauer2011word}, \cite{graesser2011coh}) either do not conform to educational guidelines or they are too broad to be useful for language learning and teaching purposes. Perhaps, the most popular text evaluation tools designed for educators is ETS's TextEvaluator \citep{napolitano2015online}. It evaluates a written text based on a number of criteria that include coherence, cohesion, lexical density and syntactic difficulty. However, their syntactic difficulty feature still does not accommodate the need of teachers. For example, if a teacher wants to provide their intermediate-level learners with a reading passage that feature the language of cause and effect, or if-conditionals, a tool like ETS's TextEvaluator does not help much, considering the categorization features they use such as average sentence length, average number of modifiers per noun phrase, average number of dependent clauses per sentence (Ibid, 2019). So in this chapter we design and implement a system that classifies a written text into three level of grammatical difficulty. The grammatical difficulty is aligned with Common European Framework Reference (CEFR) \citep{council2001common} language learning standards. 

Common European Framework Reference (CERF) is a standard for describing language achievement on a six-point scale, from A1 for beginners, up to C2 for proficient user ability on the four primary skills reading, writing, listening, speaking, and the two secondary skills: vocabulary and grammar. The latter two skills are considered the backbone for the four primary skills in terms of difficulty. For example, a frequently uncommon lexical item like sagacious makes the sentence in which it appears more difficult to an English learner than a sentence with a more frequent lexical item like wise.Similarly, grammatical structures also play a vital role in the overall difficulty of a sentence. For instance, the word \emph{should} conveys a different meaning in \emph{I should study for my final exams} than in \emph{this is your mission should you accept it}. The latter is considered more difficult to an English learner because the use of should as a conditional is less frequent than its use as a modal auxiliary expressing necessity according to CEFR. 

The English Grammar Profile (EGP) \citep{o2017english} is a finite list of (1222) grammar features compiled from the Cambridge Learner Corpus which comprises over 250,000 scripts from Cambridge English Exams at all levels. The purpose of EGP is to "establish which grammatical features characterises learners' output at each level of CEFR."(EPG, Information Booklet) The list contains structural features, their corresponding CEFR levels and example sentences (Figure 3.1)

Our goal in this chapter is to train a multinomial logistic regression classifier on the example sentences derived from EPG to predict the CEFR difficulty level. Due to the limited number of examples (around 3000 example sentences), we combine each two levels into one class, so we end up with three super-levels: A, B, and C where they indicate the levels elementary, intermediate and advanced respectively. We also improve the performance of the classification by applying a bootstrapping technique \citep{yarowsky_unsupervised_1995} to fetch data from external source to supplement the training data. Finally, we evaluate the performance of the classification before and after the bootstrapping against a random classifier.

%In this chapter, we examine the use of multinomial logistic regression classifier in a semi-supervised manner to automatically categorize English sentences based on difficulty to second language learners using the Common European Framework Reference. We also compare some of text representation techniques introduced in the previous chapter and determine the best one to use in this task. Finally, we evaluate the overall performance of the classifier before and the semi-supervised data augmentation. 
%
%Common European Framework Reference (CERF) is a standard for describing language achievement on a six-point scale, from A1 for beginners, up to C2 for proficient user ability on the four primary skills reading, writing, listening, speaking, and the two secondary skills: vocabulary and grammar. The latter two skills are considered the backbone for the four primary skills in terms of difficulty. For example, a frequently uncommon lexical item like \emph{sagacious} makes the sentence in which it appears more difficult to an English learner than a sentence with a more frequent lexical item like \emph{wise}. Similarly, grammatical structures also play a vital role in the overall difficulty of a sentence. For instance, the word \emph{should} conveys a different meaning in \emph{I should study for my final exams.} than in \emph{This is your mission should you accept it.}. The latter is considered more difficult to an English learner because the use of \emph{should} as a conditional is less frequent than its use as a modal auxiliary expressing necessity. 
%
%Wide World Web and social media are becoming indispensable resources for second language learners. However, deciding the appropriateness of materials to learners is a very labor-intensive task for English teachers. Instructors spent hours sifting through materials online to determine their difficulty and whether or not they are appropriate for their learners. To this end, we leverage the power of machine learning to build a tool that can automatically determine the difficulty of a given text according to CEFR guidelines. 
%CEFR provides 1200 criteria \citep{noauthor_english_nodate} for the difficutly of grammatical structures \ref{tab:cefr}.
\begin{table}
\caption{An excerpt of English sentences and their corresponding CEFR difficulty levels}
\centering
\begin{tabular}{l|c|c|c}
Sentence   & Level \\
\hline
I go there every year with my friends & A1 & \\
I think swimming is good for my body. & A2 & \\
Tomorrow I'm expecting a delivery of our latest\\ catalogues. & B1 & \\
Do not hesitate to contact me should \\you need further information. & B2 & \\
Living in Greece, I have had a chance \\to realise how much tourism can affect one's life. & C1 & \\
There were no photographs of \\him in Ann 's mother's albums. & C2 & \\
\end{tabular}
\label{tab:cefr}
\end{table}

%We treat this task as a multinomial classification problem. In our first attempt, we train a logistic regression classifier optimized by Newton-Raphson method on a dataset of around 3000 examples provided by [englishprofile.com], described in details in section 2. We get an F1 score of 0.67. In our second attempt, we use the trained classifier from our first attempt to predict the difficulty of sentences in an unlabeled corpus to fetch more training. After that, we merge the newly fetched data with the original dataset (obtained from Englishprofile.com) and re-train another multinomial logistic regression classifier from scratch to get an F1 score of 0.79. 

\section{Related Works}
\label{sec:research_background}

Sentence classification is the most common task in natural language processing. A considerable number of studies conducted on lexicalized tasks like sentiment analysis, spam filtering, news categorization, etc. Rarely do we see work on classification sentences based on their syntactic structures. Thus, to the best of our knowledge, no work has ever tackled the task of classifying the grammatical difficulty of English sentences according to CEFR guidelines. Therefore, we will briefly survey techniques used to solve general sentence classification problems. 

Proximity-based Algorithms such as Rocchio's algorithm \citep{rocchio1971relevance}  and K-nearest neighbor \citep{tam_comparative_2002} build vector for each class using a training set of a document by measuring the similarity, such as Euclidean distance or cosine similarity, between the documents. Some studies \citep{bang_hierarchical_2006} incorporated dictionary-based methods to construct a conceptual similarity with KNN classifier, while \citep{chang_using_2009} combine two KNN classifier with a Naive Bayes one using TF-IDF over phrases to categorize large collections of emails. While proximity-based methods perform well on document classification, yet using them on a large training set is not feasible as computing similarities across documents is computationally and resource-wise expensive. Another disadvantage is that noise, and irrelevant data can severely degrade the performance of the classification.

Another family of classifiers that work well in text categories tasks is decision trees. Classification using this method is done through automatic creating of "if-then" rules. Their use in tasks like spam filtering is widespread even with the advent of deep neural network methods \citep{wu_behavior-based_2009}. Another common powerful classifier is Naive Bayes that based on Baye's rule. It perform surprisingly well for many real-world classification applications under some specific conditions \citep{mccallum_comparison_nodate} \citep{rish_analysis_2001}. While Naive Bayes does not often outperform discriminative classifiers like Support-Vector Machine, it has the advantage of functioning well with small training data. \citep{kim_effective_2006} and \citep{isa_text_2008} show good results by selecting Naive Bayes with SVM for text classification and clustering the documents. Using a Poisson Naive Bayes for text classification model also yield excellent results \citep{isa_text_2008}.  
 
The SVM classification method and logistic regression have shown outstanding results when used in text classification tasks \citep{Yang1999ARO} \citep{brucher2002document} perhaps because of their capacity of handling high-dimensional sparse data well.  However, they can be relatively complex in training and require high time and resource consumption.



\section{Method}
\label{sec:logistic}

\subsection{Dataset: English Grammar Profile}
The dataset used for this study, provided by English Grammar Profile, exhibits typical grammar profile for each level. It consists of 3615 examples and 1215 rules, divided as follows: class A has 1194 supporting examples; class B has 1775 examples, and class C has 646 supporting examples. We merged each the six levels into three supercategories in order to provide more data within each category. 

\begin{itemize}
    \item \textbf{Rule}: Can use 'and' to join a limited range of common adjectives.
    \item \textbf{Example}: The teachers are very nice and friendly.
    \item \textbf{Level}: A1
\end{itemize}
As we can see, the dataset provides some guidance of what characterizes each example. These rules are prepared to be used by human teachers, and they are not very computationally friendly. On the other hand, the small number of examples in the dataset is not enough to build a reliable probabilistic classifier to learn to distinguish the various properties of the classes. Therefore, in order to improve the performance, we are using a semi-supervised data augmentation technique similar to Yarawsky's bootstrapping approach \citep{yarowsky_unsupervised_1995}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.75\linewidth]{../Figures/pca_proj.png} 
    \caption{2d projection of the three classes A, B and C using K-means Clustering and PCA}
    \label{fig:cmdstudy}
\end{figure}

\subsection{Multinomial Logistic Regression}
Suppose that $\mathbf{x}$ is the vectorized sentence in either of its two forms: BoW or TfIDF, $y\in Y$ is the class label, $\mathbf{w}_k$ and $b_k$ are network parameters associated with class $y$. Then the probability of $\mathbf{x}$ belonging to the class $y$ can be defined by the Softmax function:
\begin{equation}
p(y|\mathbf{x}) =\frac{1}{z(\mathbf{x})}\exp(\mathbf{w}_y^T\mathbf{x}+b_y) \,,
\label{eq:softmax}
\end{equation}
where $z(\mathbf{x}) = \sum_{j=1}^{K}\exp(\mathbf{w}_j^T\mathbf{x}+b_j)$.

Let the log-likelihood function be $$L ( \beta ) = \sum _ { i = 1 } ^ { N } \log p _ { g _ { i } } \left( x _ { i } ; \beta \right)$$. 

$$= \sum _ { i = 1 } ^ { N } \left[ \overline { \beta } _ { g _ { i } } ^ { T } x _ { i } - \log \left( 1 + \sum _ { l = 1 } ^ { K - 1 } e ^ { \overline { \beta } _ { l } ^ { T } x _ { i } } \right) \right]$$

To apply the Newton-Raphson method, we need the second derivative of the log-likelihood function

$$\frac { \partial ^ { 2 } L ( \beta ) } { \partial \beta _ { k j } \partial \beta _ { m n } } = - \sum _ { i = 1 } ^ { N } x _ { i j } x _ { i n } p _ { k } \left( x _ { i } ; \beta \right) \left[ l ( k = m ) - p _ { m } \left( x _ { i } ; \beta \right) \right]$$

The formula for updating $\beta_{new}$ for multiclass is:
$$\beta ^ { \text { new } } = \beta ^ { \text { old } } + \left( \tilde { \mathbf { X } } ^ { T } \mathbf { W } \tilde { \mathbf { X } } \right) ^ { - 1 } \tilde { \mathbf { X } } ^ { T } ( \mathbf { y } - \mathbf { p } )$$

where $y$ is the concatenated indicator vector of dimension $N \times (K-1)$, $p$ is the concatenated vector of fitted probabilities of dimension $N \times (K-1)$, $\tilde{X}$ is an $N(K-1)\times (p+1)(K-1)$ matrix; and Matrix $W$ is an $N(K-1)\times N(K-1)$ square matrix.

\subsection{Feature Design}

For this task, we are comparing two standard methods of feature generation in natural language processing and information retrieval: bag-of-word model and term-frequency inverse-document frequency (TFIDF)

%\subsubsection{Bag of Word Model} a method to represent the occurrence of words within one sentence. We add different variation by counting unigram, bigram and trigram frequencies. Besides, because this model ignores the order or the location of the word in the sentence, it might not be that helpful to our task as it is not lexicalized and the grammar is the core of it. Therefore, we replace noun and adjective words with their parts-of-speech tags in order to enforce the presence of the grammatical category and avoid the influence of the word itself.  For example, the sentence "this is your mission should you accept it” becomes this is NP should S-PRN VRB O-PRN (CLAUSE)”. 
%
%\subsubsection{TF-IDF}  This common information retrieval technique differs from the regular bag-of-words model in that length of the sentence, and the frequency of words across sentences does not influence the score of a sentence as it is normalized by other sentences in the dataset. TF·IDF is calculated based on term frequency and document frequency. Term frequency is the number of times a term $t$ occurs in a document $d$ , denoted by TF (t, d) . Document frequency measures the number
%of documents in which a term t occurs, denoted by DF(t) . TF·IDF is typically calculated as:
%
%$$T F \cdot I D F ( t , d ) = T F ( t , d ) \cdot \log \frac { | D | } { D F ( t ) }$$, where $|D|$ is the total number of documents.





\section{Experimental Results}
\label{sec:exp}

In this section, we evaluate several variations of our method against a random model as our baseline model. In the following experiments, all methods are done using Python 3.5 and Scikit Learn Library tested on MacBook Pro laptop with Core i5 processor and 8 GB of RAM.

\subsection{First Phase}

In the first phase of our experiment, we train a logistic regression classifier, optimized by the Newton-Raphson method, on English Grammar Profile dataset. We also introduce a  feature design method to mask word categories with their part-of-speech tags in order to preserve grammatical information and avoid the lexical influence of the word. We use a combination of feature design methods such as:

\begin{itemize}
    \item \textbf{BoW}: Apply unigram, bigram and trigram bag-of-words model with both word tokens, and masked tokens.
    \item \textbf{Tf-idf}: Apply unigram, bigram and trigram tf-idf model with both word tokens, and masked tokens.
\end{itemize}


\begin{figure}[t]
    \centering
    \includegraphics[width=.75\linewidth]{../Figures/conf_matrix_iter1.png} 
    \caption{Confusion matrix illustrating the performance of LR-BoW-Word model with class weight to deal with data imbalance}
    \label{fig:cmdstudy}
\end{figure}

\begin{table}
\centering
\caption {Comparison of Precision-Recall - Phase One}
\begin{tabular}{l|c|c|c}
Model  &  Precision (\%) &  Recall (\%)  & F1 (\%) \\
\hline
Random-Baseline  &  39 (\%) &  39 (\%)  & 39 (\%) \\
\textbf{BoW-LR-Words}  &  70 (\%) &  \textbf{69} (\%)  & \textbf{69} (\%) \\
BoW-LR-Masked  &  66 (\%) &  62 (\%)  & 63 (\%) \\
Tfidf-LR-Words  &  \textbf{75} (\%) &  66 (\%)  & 60 (\%) \\
Tfidf-LR-Masked  &  66 (\%) &  64 (\%)  & 61 (\%) \\
\hline

\end{tabular}
\label {tb:flickre2e}
\end{table}

From table 2, we compare the performance of 4 variations of our method against the baseline, which is a random model. Surprisingly, we notice that our bag-of-words model with word tokens (BoW-LR-Words) outperform the one with the masked sequence in terms of precision and recall respectively. Besides, BoW-LR-Words also outperforms (tfidf-LR-words) model in terms of recall only, while the latter has higher recall. From the confusion matrix (fig. 2), we see that (BoW-LR-Words) model predicts most of the testing data correctly even though the number of training data is not equal among the classes, especially for class C. Therefore, we needed to assign specific weights to each class during the training in order to avoid this imbalance in the data. 

\subsection{Second Phase}

The results shown in Table 2 does indicate that the linear model has learned the associations between sentences and their corresponding grammatical class reasonably well. However, both of our feature design techniques, namely bag-of-words and tfidf have their disadvantages. The first assigns similarity based on occurrence, size and mutual words. For example, the BoW model learns that longer sentences tend to be the most difficult, while shorter ones are less difficult. Similarly, while tfidf treats the drawbacks of BoW model, it still ignores the usefulness of what-so-called \textit{Stop Words} due to their commonality. In other words, it is very hard to trust these results with this amount of training data. Therefore, we propose a non-synthetic data augmentation method to provide more training examples, and thereby improve the overall performance of the model. 

Using a text-only version of the brown corpus \citep{citeulike: 13797746}, we use our BoW-LR-Words model to predict the grammatical difficulty labels of its sentences. Then, we collect the sentences predicted with high certainty (above 0.9 probability) to be fed to the original dataset. It is crucial to set a higher probability threshold as the new data examples will serve as seed points for our prediction in the future, and we do not want the classifier to \textit{drift} from the original trajectory. This technique is reminiscent of Yarawsky's Bootstrapping \citep{yarowsky_unsupervised_1995}(semi-supervised) algorithm, but with one difference is that unlike in Yarawsky's method, we apply this step only once. 

Out of 38400 sentences in the unlabeled corpus, only 1428 sentences have been detected with certainty above 90\%. We also removed any sentence that is longer than 30 words in order to lessen the undesirable effect of BoW technique. We apply our best model from phase one to the augmented dataset of 5043 examples under the same training conditions to get a precision of \textbf{0.80}, recall of \textbf{0.79}, and F1 score of \textbf{0.79}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.75\linewidth]{../Figures/conf_matrix_iter2.png} 
    \caption{Confusion matrix illustrating the performance of LR-BoW-Word model after augmentation}
    \label{fig:cmdstudy}
\end{figure}

\section {Conclusion}
\label{sec:conclusion}
In this chapter, we have presented a classification method based on simple multinomial logistic regression and bag-of-words model augmented with semi-supervised (bootstrapping) method to classify English sentences based on their level difficulty, A=Beginner, B=intermediate, C=Advanced according to CEFR. We have also compared several common feature design techniques to encode the sentences along with a proposed masking BoW method that replaces lexical items with their grammatical category. Finally, the model achieves an overall F1 score of 0.69, and 0.79 after augmenting it with example sentences from an unlabeled corpus.
