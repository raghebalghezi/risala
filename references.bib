@article{fuhr1991probabilistic,
  title={A probabilistic learning approach for document indexing},
  author={Fuhr, Norbert and Buckley, Chris},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={9},
  number={3},
  pages={223--248},
  year={1991},
  publisher={ACM}
}

@inproceedings{tzeras1993automatic,
  title={Automatic indexing based on Bayesian inference networks},
  author={Tzeras, Kostas and Hartmann, Stephan},
  booktitle={Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={22--35},
  year={1993},
  organization={ACM}
}

@phdthesis{lewis1992representation,
  title={Representation and learning in information retrieval},
  author={Lewis, David Dolan},
  year={1992},
  school={University of Massachusetts at Amherst}
}

@article{caropreso2001learner,
  title={A learner-independent evaluation of the usefulness of statistical phrases for automated text categorization},
  author={Caropreso, Maria Fernanda and Matwin, Stan and Sebastiani, Fabrizio},
  journal={Text databases and document management: Theory and practice},
  volume={5478},
  pages={78--102},
  year={2001},
  publisher={Citeseer}
}

@misc{schmid1994probabilistic,
  title={Probabilistic part-of-speech tagging using decision trees. In proceedings of international conference on new methods in language processing,(pp. 1-9), Access date: 09.11. 2012},
  author={Schmid, Helmut},
  year={1994}
}

@article{porter1997readings,
  title={Readings in information retrieval},
  author={Porter, Martin F},
  year={1997},
  publisher={Morgan Kaufmann Publishers Inc.}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Wiley Online Library}
}

@inproceedings{slonim2001power,
  title={The power of word clusters for text classification},
  author={Slonim, Noam and Tishby, Naftali},
  booktitle={23rd European Colloquium on Information Retrieval Research},
  volume={1},
  pages={200},
  year={2001}
}

@inproceedings{sable2001using,
  title={Using bins to empirically estimate term weights for text categorization},
  author={Sable, Carl and Church, Kenneth W},
  booktitle={Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing},
  year={2001}
}

@inproceedings{baker1998distributional,
  title={Distributional clustering of words for text classification},
  author={Baker, L Douglas and McCallum, Andrew Kachites},
  booktitle={Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={96--103},
  year={1998},
  organization={ACM}
}

@article{deerwester1990indexing,
  title={Indexing by latent semantic analysis},
  author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  journal={Journal of the American society for information science},
  volume={41},
  number={6},
  pages={391--407},
  year={1990},
  publisher={Wiley Online Library}
}

@article{porter1980algorithm,
  title={An algorithm for suffix stripping},
  author={Porter, Martin F},
  journal={Program},
  volume={14},
  number={3},
  pages={130--137},
  year={1980},
  publisher={MCB UP Ltd}
}

@inproceedings{robertson1997relevance,
  title={On relevance weights with little relevance information},
  author={Robertson, Stephen E and Walker, Steve},
  booktitle={ACM SIGIR Forum},
  volume={31},
  number={SI},
  pages={16--24},
  year={1997},
  organization={ACM}
}

@inproceedings{singhal1997learning,
  title={Learning routing queries in a query zone},
  author={Singhal, Amit and Mitra, Mandar and Buckley, Chris},
  booktitle={ACM SIGIR Forum},
  volume={31},
  number={SI},
  pages={25--32},
  year={1997},
  organization={ACM}
}

@book{bellman2015adaptive,
  title={Adaptive control processes: a guided tour},
  author={Bellman, Richard E},
  volume={2045},
  year={2015},
  publisher={Princeton university press}
}

@article{firth1957synopsis,
  title={A synopsis of linguistic theory, 1930-1955},
  author={Firth, John R},
  journal={Studies in linguistic analysis},
  year={1957},
  publisher={Basil Blackwell}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@book{jurafsky2014speech,
  title={Speech and language processing},
  author={Jurafsky, Dan and Martin, James H},
  volume={3},
  year={2014},
  publisher={Pearson London}
}

@misc{noauthor_english_nodate,
  title = {English {Profile} - {EGP} {Online}},
  url = {http://www.englishprofile.org/english-grammar-profile/egp-online},
  urldate = {2019-01-29},
  file = {English Profile - EGP Online:/Users/raghebal-ghezi/Zotero/storage/CDDGDZ7R/egp-online.html:text/html}
}


@article{chang_using_2009,
  title = {Using phrases as features in email classification},
  volume = {82},
  issn = {0164-1212},
  url = {http://www.sciencedirect.com/science/article/pii/S0164121209000089},
  doi = {10.1016/j.jss.2009.01.013},
  abstract = {In this paper, we report our experience on the use of phrases as basic features in the email classification problem. We performed extensive empirical evaluation using our large email collections and tested with three text classification algorithms, namely, a naive Bayes classifier and two k-NN classifiers using TF–IDF weighting and resemblance respectively. The investigation includes studies on the effect of phrase size, the size of local and global sampling, the neighbourhood size, and various methods to improve the classification accuracy. We determined suitable settings for various parameters of the classifiers and performed a comparison among the classifiers with their best settings. Our result shows that no classifier dominates the others in terms of classification accuracy. Also, we made a number of observations on the special characteristics of emails. In particular, we observed that public emails are easier to classify than private ones.},
  number = {6},
  urldate = {2018-12-09},
  journal = {Journal of Systems and Software},
  author = {Chang, Matthew and Poon, Chung Keung},
  month = jun,
  year = {2009},
  keywords = {Document classification, Email, Naive Bayes, Nearest-neighbour, Resemblance},
  pages = {1036--1045},
  file = {ScienceDirect Snapshot:/Users/raghebal-ghezi/Zotero/storage/L7BXDP32/S0164121209000089.html:text/html}
}

@article{rocchio1971relevance,
  title={Relevance feedback in information retrieval},
  author={Rocchio, Joseph John},
  journal={The SMART retrieval system: experiments in automatic document processing},
  pages={313--323},
  year={1971},
  publisher={Prentice-Hall Inc.}
}

@inproceedings{tam_comparative_2002,
  title = {A comparative study of centroid-based, neighborhood-based and statistical approaches for effective document categorization},
  volume = {4},
  doi = {10.1109/ICPR.2002.1047440},
  abstract = {Associating documents to relevant categories is critical for effective document retrieval. Here, we compare the well-known k-nearest neighborhood (kNN) algorithm, the centroid-based classifier and the highest average similarity over retrieved documents (HASRD) algorithm, for effective document categorization. We use various measures such as the micro and macro F1 values to evaluate their performance on the Reuters-21578 corpus. The empirical results show that kNN performs the best, followed by our adapted HASRD and the centroid-based classifier for common document categories, while the centroid-based classifier and kNN outperform our adapted HASRD for rare document categories. Additionally, our study clearly indicates that each classifier performs optimally only when a suitable term weighting scheme is used All these significant results lead to many exciting directions for future exploration.},
  booktitle = {Object recognition supported by user interaction for service robots},
  author = {Tam, V. and Santoso, A. and Setiono, R.},
  month = aug,
  year = {2002},
  keywords = {Bayesian methods, centroid-based document categorization, classification, document retrieval, Extraterrestrial measurements, Frequency, HASRD algorithm, highest average similarity algorithm, information retrieval, Information retrieval, Internet, k-nearest neighborhood algorithm, kNN algorithm, macro F1 values, micro F1 values, Nearest neighbor searches, neighborhood-based document categorization, optimal classification, Performance analysis, Reuters-21578 corpus, Software libraries, statistical analysis, Statistical analysis, statistical document categorization, term weighting scheme, Testing},
  pages = {235--238 vol.4},
  file = {IEEE Xplore Abstract Record:/Users/raghebal-ghezi/Zotero/storage/2Z8EUR45/1047440.html:text/html}
}

@article{bang_hierarchical_2006,
  title = {Hierarchical document categorization with k-{NN} and concept-based thesauri},
  volume = {42},
  issn = {0306-4573},
  url = {http://www.sciencedirect.com/science/article/pii/S0306457305000579},
  doi = {10.1016/j.ipm.2005.04.003},
  abstract = {In this paper, we propose a new algorithm, which incorporates the relationships of concept-based thesauri into the document categorization using the k-NN classifier (k-NN). k-NN is one of the most popular document categorization methods because it shows relatively good performance in spite of its simplicity. However, it significantly degrades precision when ambiguity arises, i.e., when there exist more than one candidate category to which a document can be assigned. To remedy the drawback, we employ concept-based thesauri in the categorization. Employing the thesaurus entails structuring categories into hierarchies, since their structure needs to be conformed to that of the thesaurus for capturing relationships between categories. By referencing various relationships in the thesaurus corresponding to the structured categories, k-NN can be prominently improved, removing the ambiguity. In this paper, we first perform the document categorization by using k-NN and then employ the relationships to reduce the ambiguity. Experimental results show that this method improves the precision of k-NN up to 13.86\% without compromising its recall.},
  number = {2},
  urldate = {2018-12-09},
  journal = {Information Processing \& Management},
  author = {Bang, Sun Lee and Yang, Jae Dong and Yang, Hyung Jeong},
  month = mar,
  year = {2006},
  keywords = {-NN, Classification, Document categorization, Thesaurus},
  pages = {387--406},
  file = {ScienceDirect Snapshot:/Users/raghebal-ghezi/Zotero/storage/PLELP2IW/S0306457305000579.html:text/html}
}

@inproceedings{apte_towards_1994,
  title = {Towards {Language} {Independent} {Automated} {Learning} of {Text} {Categorization} {Models}},
  isbn = {978-1-4471-2099-5},
  abstract = {We describe the results of extensive machine learning experiments on large collections of Reuters’ English and German newswires. The goal of these experiments was to automatically discover classification patterns that can be used for assignment of topics to the individual newswires. Our results with the English newswire collection show a very large gain in performance as compared to published benchmarks, while our initial results with the German newswires appear very promising. We present our methodology, which seems to be insensitive to the language of the document collections, and discuss issues related to the differences in results that we have obtained for the two collections.},
  language = {en},
  booktitle = {{SIGIR} ’94},
  publisher = {Springer London},
  author = {Apté, Chidanand and Damerau, Fred and Weiss, Sholom M.},
  editor = {Croft, Bruce W. and van Rijsbergen, C. J.},
  year = {1994},
  keywords = {Breakeven Point, Document Classification, News Story, Rule Induction, Training Case},
  pages = {23--30}
}

@article{article,
author = {Apte, C and Damerau, F and Weiss, Sholom and Sholom, Damerau and Weiss, M},
year = {1999},
month = {10},
pages = {},
title = {Towards Language Independent Automated Learning of Text Categorization Models},
journal = {Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval}
}

@article{apte_automated_1994,
  title = {Automated {Learning} of {Decision} {Rules} for {Text} {Categorization}},
  volume = {12},
  abstract = {We describe the results of extensive experiments on large document collections using optimized  rule-based induction methods. The goal of these methods is to automatically discover  classification patterns that can be used for general document categorization or personalized filtering  of free text. Previous reports indicate that human-engineered rule-based systems, requiring  manymanyears of developmental efforts, have been successfully built to "read" documents and  assign topics to them. In this paper, weshowthatmachine generated decision rules appear  comparable to human performance, while using the identical rule-based representation. In comparison  with other machine learning techniques, results on a key benchmark from the Reuters  collection show a large gain in performance, from a previously reported 65\% recall/precision  breakeven point to 80.5\%. In the context of a very high dimensional feature space, several  methodological alternatives are examined, including universal versu...},
  journal = {ACM Transactions on Information Systems},
  author = {Apte, C. and Damerau, F. and Weiss, S. M. and Apte, Chidanand and Damerau, Fred and Weiss, Sholom M.},
  year = {1994},
  pages = {233--251},
  file = {Citeseer - Full Text PDF:/Users/raghebal-ghezi/Zotero/storage/59T38J3V/Apte et al. - 1994 - Automated Learning of Decision Rules for Text Cate.pdf:application/pdf;Citeseer - Snapshot:/Users/raghebal-ghezi/Zotero/storage/DYJPXB6A/summary.html:text/html}
}

@article{wu_behavior-based_2009,
  title = {Behavior-based spam detection using a hybrid method of rule-based techniques and neural networks},
  volume = {36},
  issn = {0957-4174},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417408001772},
  doi = {10.1016/j.eswa.2008.03.002},
  abstract = {Earlier methods on spam filtering usually compare the contents of emails against specific keywords, which are not robust as the spammers frequently change the terms used in emails. This paper presents a hybrid method of rule-based processing and back-propagation neural networks for spam filtering. Instead of using keywords, this study utilize the spamming behaviors as features for describing emails. A rule-based process is first employed to identify and digitize the spamming behaviors observed from the headers and syslogs of emails. An enhanced BPNN with a weighted learning strategy is designed as the classification mechanism. Since spamming behaviors are infrequently changed, compared with that of keywords used in spams, the proposed method is more robust with respect to the change of time. The experimental results show that the proposed method is useful in identifying spam emails.},
  number = {3, Part 1},
  urldate = {2018-12-09},
  journal = {Expert Systems with Applications},
  author = {Wu, Chih-Hung},
  month = apr,
  year = {2009},
  keywords = {Classification, Back-propagation neural networks, Machine-learning, Rule-based reasoning, Spam filtering},
  pages = {4321--4330},
  file = {ScienceDirect Snapshot:/Users/raghebal-ghezi/Zotero/storage/Z38ABUAD/S0957417408001772.html:text/html}
}

@article{isa_text_2008,
  title = {Text {Document} {Preprocessing} with the {Bayes} {Formula} for {Classification} {Using} the {Support} {Vector} {Machine}},
  volume = {20},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2008.76},
  abstract = {This work implements an enhanced hybrid classification method through the utilization of the naive Bayes approach and the support vector machine (SVM). In this project, the Bayes formula was used to vectorize (as opposed to classify) a document according to a probability distribution reflecting the probable categories that the document may belong to. The Bayes formula gives a range of probabilities to which the document can be assigned according to a predetermined set of topics (categories) such as those found in the "20 Newsgroups" data set for instance. Using this probability distribution as the vectors to represent the document, the SVM can then be used to classify the documents on a multidimensional level. The effects of an inadvertent dimensionality reduction caused by classifying using only the highest probability using the naive Bayes classifier can be overcome using the SVM by employing all the probability values associated with every category for each document. This method can be used for any data set and shows a significant reduction in training time as compared to the Lsquare method and significant improvement in the classification accuracy when compared to pure naive Bayes systems and also the TF-IDF/SVM hybrids.},
  number = {9},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  author = {Isa, D. and Lee, L. H. and Kallimani, V. P. and RajKumar, R.},
  month = sep,
  year = {2008},
  keywords = {classification, Applications {\textless}, Bayes methods, Classification tree analysis, Classifier design and evaluation {\textless}, Classifier design and evaluation {\textless}Design Methodology {\textless}Pattern Recognition {\textless}Computing Methodologies, Computing Methodologies, Decision trees, Design Methodology {\textless}, Document analysis, Document and Text Processing {\textless}, Document and Text Processing {\textless}Computing Methodologies, Electronic mail, Filtering, Instruction sets, Multidimensional systems, naive Bayes classifier, pattern classification, Pattern Recognition {\textless}, probability distribution, Probability distribution, statistical distributions, support vector machine, Support vector machine classification, support vector machines, Support vector machines, text analysis, Text categorization, text document preprocessing, Text processing {\textless}, Text processing {\textless}Applications {\textless}Pattern Recognition {\textless}Computing Methodologies},
  pages = {1264--1272},
  file = {IEEE Xplore Abstract Record:/Users/raghebal-ghezi/Zotero/storage/8Y5X5NRJ/4492780.html:text/html}
}

@article{isa_using_2009,
  title = {Using the self organizing map for clustering of text documents},
  volume = {36},
  issn = {0957-4174},
  url = {http://www.sciencedirect.com/science/article/pii/S0957417408004879},
  doi = {10.1016/j.eswa.2008.07.082},
  abstract = {An increasing number of computational and statistical approaches have been used for text classification, including nearest-neighbor classification, naïve Bayes classification, support vector machines, decision tree induction, rule induction, and artificial neural networks. Among these approaches, naïve Bayes classifiers have been widely used because of its simplicity. Due to the simplicity of the Bayes formula, the naïve Bayes classification algorithm requires a relatively small number of training data and shorter time in both the training and classification stages as compared to other classifiers. However, a major short coming of this technique is the fact that the classifier will pick the highest probability category as the one to which the document is annotated too. Doing this is tantamount to classifying using only one dimension of a multi-dimensional data set. The main aim of this work is to utilize the strengths of the self organizing map (SOM) to overcome the inadvertent dimensionality reduction resulting from using only the Bayes formula to classify. Combining the hybrid system with new ranking techniques further improves the performance of the proposed document classification approach. This work describes the implementation of an enhanced hybrid classification approach which affords a better classification accuracy through the utilization of two familiar algorithms, the naïve Bayes classification algorithm which is used to vectorize the document using a probability distribution and the self organizing map (SOM) clustering algorithm which is used as the multi-dimensional unsupervised classifier.},
  number = {5},
  urldate = {2018-12-09},
  journal = {Expert Systems with Applications},
  author = {Isa, Dino and Kallimani, V. P. and Lee, Lam Hong},
  month = jul,
  year = {2009},
  keywords = {Bayesian, Clusters similarity, Self organizing maps},
  pages = {9584--9591},
  file = {ScienceDirect Snapshot:/Users/raghebal-ghezi/Zotero/storage/WBZ8BGUX/S0957417408004879.html:text/html}
}

@article{kim_effective_2006,
  title = {Some {Effective} {Techniques} for {Naive} {Bayes} {Text} {Classification}},
  volume = {18},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2006.180},
  abstract = {While naive Bayes is quite effective in various data mining tasks, it shows a disappointing result in the automatic text classification problem. Based on the observation of naive Bayes for the natural language text, we found a serious problem in the parameter estimation process, which causes poor results in text classification domain. In this paper, we propose two empirical heuristics: per-document text normalization and feature weighting method. While these are somewhat ad hoc methods, our proposed naive Bayes text classifier performs very well in the standard benchmark collections, competing with state-of-the-art text classifiers based on a highly complex learning method such as SVM},
  number = {11},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  author = {Kim, Sang-Bum and Han, Kyoung-Soo and Rim, Hae-Chang and Myaeng, Sung Hyon},
  month = nov,
  year = {2006},
  keywords = {Data mining, classification, Frequency, Bayes methods, naive Bayes classifier, Support vector machine classification, Support vector machines, text analysis, Text categorization, automatic text classification problem, data mining, data mining tasks, feature weighting method, feature weighting., learning (artificial intelligence), Learning systems, naive Bayes text classification, natural language, natural language processing, Natural languages, parameter estimation, Parameter estimation, parameter estimation process, per-document text normalization, Poisson model, Probability, Statistical learning, Text classification},
  pages = {1457--1466},
  file = {IEEE Xplore Abstract Record:/Users/raghebal-ghezi/Zotero/storage/5NXR2W5U/1704799.html:text/html}
}

@article{brucher2002document,
  title={Document classification methods for organizing explicit knowledge},
  author={Br{\"u}cher, Heide and Knolmayer, Gerhard and Mittermayer, Marc-Andr{\'e}},
  year={2002},
  publisher={Institute of Information Systems}
}

@article{mccallum_comparison_nodate,
  title = {A {Comparison} of {Event} {Models} for {Naive} {Bayes} {Text} {Classiﬁcation}},
  abstract = {Recent approaches to text classiﬁcation have used two diﬀerent ﬁrst-order probabilistic models for classiﬁcation, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the diﬀerences and details of these two models, and by empirically comparing their classiﬁcation performance on ﬁve text corpora. We ﬁnd that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes—providing on average a 27\% reduction in error over the multi-variate Bernoulli model at any vocabulary size.},
  language = {en},
  author = {McCallum, Andrew and Nigam, Kamal},
  pages = {8},
  file = {McCallum and Nigam - A Comparison of Event Models for Naive Bayes Text .pdf:/Users/raghebal-ghezi/Zotero/storage/YTVETFVI/McCallum and Nigam - A Comparison of Event Models for Naive Bayes Text .pdf:application/pdf}
}

@article{article,
author = {Rish, Irina},
year = {2001},
month = {01},
pages = {},
title = {An Empirical Study of the Naïve Bayes Classifier},
volume = {3},
journal = {IJCAI 2001 Work Empir Methods Artif Intell}
}

@techreport{rish_analysis_2001,
  title = {An analysis of data characteristics that affect naive {Bayes} performance},
  abstract = {Despite its unrealistic independence assumption, the naive Bayes classifier is remarkably successful in practice. This paper identifies some data characteristics for which naive Bayes works well, such as certain deterministic and almostdeterministic dependencies (i.e., low-entropy distributions). First, we address zero-Bayesrisk problems, proving naive Bayes optimality for any two-class concept that assigns class},
  author = {Rish, Irina and Hellerstein, Joseph and Thathachar, Jayram},
  year = {2001},
  file = {Citeseer - Full Text PDF:/Users/raghebal-ghezi/Zotero/storage/8ZBQJN9T/Rish et al. - 2001 - An analysis of data characteristics that affect na.pdf:application/pdf;Citeseer - Snapshot:/Users/raghebal-ghezi/Zotero/storage/QU8YAB8T/summary.html:text/html}
}

@inproceedings{kim_effective_2002,
  series = {Lecture {Notes} in {Computer} {Science}},
  title = {Effective {Methods} for {Improving} {Naive} {Bayes} {Text} {Classifiers}},
  isbn = {978-3-540-45683-4},
  abstract = {Though naive Bayes text classifiers are widely used because of its simplicity, the techniques for improving performances of these classifiers have been rarely studied. In this paper, we propose and evaluate some general and effective techniques for improving performance of the naive Bayes text classifier. We suggest document model based parameter estimation and document length normalization to alleviate the problems in the traditional multinomial approach for text classification. In addition, Mutual-Information-weighted naive Bayes text classifier is proposed to increase the effect of highly informative words. Our techniques are evaluated on the Reuters21578 and 20 Newsgroups collections, and significant improvements are obtained over the existing multinomial naive Bayes approach.},
  language = {en},
  booktitle = {{PRICAI} 2002: {Trends} in {Artificial} {Intelligence}},
  publisher = {Springer Berlin Heidelberg},
  author = {Kim, Sang-Bum and Rim, Hae-Chang and Yook, DongSuk and Lim, Heui-Seok},
  editor = {Ishizuka, Mitsuru and Sattar, Abdul},
  year = {2002},
  keywords = {Document Length, Feature Word, Relevance Score, Short Document, Training Document},
  pages = {414--423}
}

@inproceedings{inproceedings,
author = {Kim, Sang-Bum and Rim, Hae-Chang and Yook, Dongsuk and Lim, Heui-Seok},
year = {2002},
month = {08},
pages = {414-423},
title = {Effective Methods for Improving Naive Bayes Text Classifiers},
volume = {2417},
doi = {10.1007/3-540-45683-X_45}
}

@inproceedings{yarowsky_unsupervised_1995,
  address = {Cambridge, Massachusetts, USA},
  title = {Unsupervised word sense disambiguation rivaling supervised methods},
  url = {http://www.aclweb.org/anthology/P95-1026},
  doi = {10.3115/981658.981684},
  urldate = {2018-12-09},
  booktitle = {Proceedings of the 33rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
  publisher = {Association for Computational Linguistics},
  author = {Yarowsky, David},
  month = jun,
  year = {1995},
  pages = {189--196},
  file = {Full Text PDF:/Users/raghebal-ghezi/Zotero/storage/773ZN3AB/Yarowsky - 1995 - UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SU.pdf:application/pdf}
}

@inproceedings{Yang1999ARO,
  title={A Re-Examination of Text Categorization Methods},
  author={Yiming Yang and Xin Liu},
  booktitle={SIGIR},
  year={1999}
}

@article{article,
author = {Yang, Yiming and Liu, Xin},
year = {2003},
month = {01},
pages = {},
title = {A Re-Examination of Text Categorization Methods},
journal = {Proceedings of the 22nd SIGIR, New York, NY, USA},
doi = {10.1145/312624.312647}
}

@misc{citeulike:13797746,
    author = {Francis, W. Nelson and Kucera, Henry},
    citeulike-article-id = {13797746},
    keywords = {proposal},
    note = {Brown University Liguistics Department},
    posted-at = {2015-10-08 17:40:45},
    priority = {2},
    title = {The Brown Corpus: A Standard Corpus of {Present-Day} Edited American English},
    year = {1979}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{DBLP:journals/corr/abs-1806-03822,
  author    = {Pranav Rajpurkar and
               Robin Jia and
               Percy Liang},
  title     = {Know What You Don't Know: Unanswerable Questions for SQuAD},
  journal   = {CoRR},
  volume    = {abs/1806.03822},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.03822},
  archivePrefix = {arXiv},
  eprint    = {1806.03822},
  timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1806-03822},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SeoKFH16,
  author    = {Min Joon Seo and
               Aniruddha Kembhavi and
               Ali Farhadi and
               Hannaneh Hajishirzi},
  title     = {Bidirectional Attention Flow for Machine Comprehension},
  journal   = {CoRR},
  volume    = {abs/1611.01603},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01603},
  archivePrefix = {arXiv},
  eprint    = {1611.01603},
  timestamp = {Mon, 13 Aug 2018 16:46:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SeoKFH16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/LevySCZ17,
  author    = {Omer Levy and
               Minjoon Seo and
               Eunsol Choi and
               Luke Zettlemoyer},
  title     = {Zero-Shot Relation Extraction via Reading Comprehension},
  journal   = {CoRR},
  volume    = {abs/1706.04115},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.04115},
  archivePrefix = {arXiv},
  eprint    = {1706.04115},
  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LevySCZ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1808-05759,
  author    = {Minghao Hu and
               Furu Wei and
               Yuxing Peng and
               Zhen Huang and
               Nan Yang and
               Ming Zhou},
  title     = {Read + Verify: Machine Reading Comprehension with Unanswerable Questions},
  journal   = {CoRR},
  volume    = {abs/1808.05759},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.05759},
  archivePrefix = {arXiv},
  eprint    = {1808.05759},
  timestamp = {Sun, 02 Sep 2018 15:01:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-05759},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1811-11934,
  author    = {Wei Wang and
               Ming Yan and
               Chen Wu},
  title     = {Multi-granularity hierarchical attention fusion networks for reading
               comprehension and question answering},
  journal   = {CoRR},
  volume    = {abs/1811.11934},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.11934},
  archivePrefix = {arXiv},
  eprint    = {1811.11934},
  timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-11934},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/VaswaniSPUJGKP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-04805},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Peters:2018,
  author={Peters, Matthew E. and  Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title={Deep contextualized word representations},
  booktitle={Proc. of NAACL},
  year={2018}
}

@article{kitaev2018constituency,
  title={Constituency Parsing with a Self-Attentive Encoder},
  author={Kitaev, Nikita and Klein, Dan},
  journal={arXiv preprint arXiv:1805.01052},
  year={2018}
}

@book{manning1999foundations,
  title={Foundations of statistical natural language processing},
  author={Manning, Christopher D and Manning, Christopher D and Sch{\"u}tze, Hinrich},
  year={1999},
  publisher={MIT press}
}